# ğŸš€ Kaggle Training - Quick Start

## ğŸ“¦ Setup (2 Minutes)

### Step 1: Clone Repository in Kaggle

Open a Kaggle notebook and run:

```bash
# Clone your repository
!git clone https://github.com/mithun1203/SGCL.git
%cd SGCL
```

That's it! All files are ready.

### Step 2: Install Dependencies

```bash
!pip install -q transformers peft accelerate bitsandbytes sentencepiece
```

## ğŸ¯ Training Scripts

### Option 1: SG-CL Training (Our Method)

```bash
# In Kaggle notebook cell
!python training/kaggle_train_sgcl.py
```

**What it does:**
- Trains Phi-3-mini with SG-CL algorithm
- Uses SID for conflict detection
- Generates guard-rails during training
- Saves checkpoints to `sgcl_checkpoints/`

**Expected output:**
```
ğŸ“¦ Installing required packages...
ğŸ–¥ï¸  Device: Tesla T4
GPU Available: True
GPU Memory: 15.11 GB

âš™ï¸  Configuration:
  Model: microsoft/Phi-3-mini-4k-instruct
  Batch size: 4
  Epochs per task: 3
  SID enabled: True
  Guard-rails enabled: True

ğŸ“ STARTING SG-CL TRAINING
======================================================================
Training Task 0...
  Step 10 | Loss: 2.4521 | Conflicts: 23 | Guard-rails: 87
  Step 20 | Loss: 2.1834 | Conflicts: 19 | Guard-rails: 72
  ...
âœ… Task 0 complete! (avg loss: 1.8234)

Training Task 1...
  ...

âœ… SG-CL TRAINING COMPLETE!
```

### Option 2: Baseline Training (For Comparison)

```bash
# In Kaggle notebook cell
!python training/kaggle_train_baselines.py
```

**What it does:**
- Trains 3 baseline methods sequentially:
  1. Naive Fine-tuning (no CL)
  2. EWC (regularization-based)
  3. Replay Buffer (memory-based)
- Each uses same configuration as SG-CL
- Saves separate checkpoints for each

**Expected output:**
```
ğŸ”µ BASELINE 1: NAIVE FINE-TUNING
======================================================================
Training Task 0...
  Step 10 | Loss: 2.4823
  ...
âœ… Naive fine-tuning complete!

ğŸŸ¢ BASELINE 2: EWC
======================================================================
Computing Fisher Information Matrix...
Training Task 0...
  ...
âœ… EWC training complete!

ğŸŸ¡ BASELINE 3: REPLAY BUFFER
======================================================================
Training Task 0...
  Storing 100 samples in replay buffer
  ...
âœ… Replay buffer training complete!

ğŸ“Š BASELINE TRAINING SUMMARY
Final task average losses:
  Naive       : 2.8934
  EWC         : 2.3421
  Replay      : 2.1876
```

## âš™ï¸ Kaggle Notebook Setup

### Complete Notebook Template

```python
# Cell 1: Setup
!git clone https://github.com/mithun1203/SGCL.git
%cd SGCL
!pip install -q transformers peft accelerate bitsandbytes sentencepiece

# Cell 2: Run SG-CL Training
!python training/kaggle_train_sgcl.py

# Cell 3 (Optional): Run Baseline Comparison
# !python training/kaggle_train_baselines.py

# Cell 4: Zip and Download Results
!zip -r sgcl_results.zip sgcl_checkpoints/
# Download from Kaggle output panel
```

That's the entire setup! No file uploads needed.

## ğŸ“Š Expected Training Time

**On Kaggle T4 GPU (15GB VRAM):**

- **SG-CL**: ~2.5 hours (5 tasks Ã— 3 epochs with conflict detection)
- **Baselines**: ~6 hours total (3 methods Ã— 2 hours each)

**Memory usage:**
- Phi-3-mini (3.8B params) + LoRA: ~6GB
- Batch size 4 + gradient accumulation: ~8GB
- Peak memory: ~10GB (safe for T4)

## ğŸ”§ If You Hit Memory Errors

Edit configuration in the training scripts:

```python
# Reduce batch size
config = TrainingConfig(
    batch_size=2,  # was 4
    gradient_accumulation_steps=8,  # was 4
    # ... rest stays same
)
```

## ğŸ“ˆ Monitoring Training

Kaggle will show real-time output:
- Loss curves
- Conflicts detected (SG-CL only)
- Guard-rails generated (SG-CL only)
- GPU memory usage
- Step timing

## ğŸ’¾ Output Files

After training completes:

### SG-CL:
```
sgcl_checkpoints/
â”œâ”€â”€ task_0/
â”‚   â””â”€â”€ adapter_model.bin  (LoRA weights)
â”œâ”€â”€ task_1/
â”œâ”€â”€ task_2/
â”œâ”€â”€ task_3/
â”œâ”€â”€ task_4/
â”œâ”€â”€ final_model/
â””â”€â”€ training_summary.json  (metrics)
```

### Baselines:
```
naive_checkpoints/    (same structure)
ewc_checkpoints/      (same structure)
replay_checkpoints/   (same structure)
```

## ğŸ¯ Success Criteria

Training is successful if:
- âœ… All 5 tasks complete without errors
- âœ… Loss decreases over epochs (< 2.0 final loss)
- âœ… SG-CL detects conflicts (>0 per batch)
- âœ… Guard-rails generated (>0 per conflict)
- âœ… Checkpoints saved for all tasks

## ğŸ› Troubleshooting

**Error: CUDA out of memory**
â†’ Reduce `batch_size` to 2 or 1

**Error: Cannot load dataset**
â†’ Check `seca_10k_final.json` is in `sid/` directory (should be auto-cloned)

**Error: Module not found (SID/Guardrail)**
â†’ Make sure you ran `%cd SGCL` to enter the repository directory

**Slow training (>5 hours)**
â†’ Normal for first run; Kaggle caches model after download

## ğŸ“ Need Help?

Check these files:
- `training/README.md` - Full documentation
- `training/sgcl_trainer.py` - Implementation details
- `training/baseline_trainers.py` - Baseline implementations

---

**Status**: Ready to deploy on Kaggle GPU ğŸš€
