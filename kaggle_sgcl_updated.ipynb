{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547252f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check GPU availability\n",
    "import torch\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install dependencies\n",
    "!pip install -q transformers peft accelerate sentence-transformers matplotlib seaborn pandas scipy\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd433d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Clone SGCL project (fresh copy)\n",
    "!cd /kaggle/working && rm -rf SGCL\n",
    "!git clone https://github.com/mithun1203/SGCL.git\n",
    "%cd SGCL\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Quick verification test (~5 min)\n",
    "# Tests with GPT-2 on mini dataset (2 tasks, 4 samples)\n",
    "print(\"üöÄ Running quick verification test...\")\n",
    "!python run_full_experiments.py --quick\n",
    "print(\"\\n‚úì Quick test completed! System is working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Run FULL experiments with Phi-3 (~1-2 hours)\n",
    "# Trains 4 methods on full SeCA dataset\n",
    "print(\"üöÄ Starting full experiments with Phi-3...\")\n",
    "print(\"‚è±Ô∏è  Estimated time: 1-2 hours on GPU T4\")\n",
    "print(\"üìä Training 4 methods: SG-CL, Naive, EWC, Replay\")\n",
    "print(\"\\nNote: ConceptNet API errors (502) are harmless - SID uses rule-based fallback\\n\")\n",
    "\n",
    "!python run_full_experiments.py --model microsoft/phi-3-mini-4k-instruct\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Generate publication-quality plots\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Find the latest experiment directory\n",
    "exp_dirs = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))\n",
    "if not exp_dirs:\n",
    "    print(\"‚ùå No experiment results found. Run cell 5 first.\")\n",
    "else:\n",
    "    latest = exp_dirs[-1]\n",
    "    print(f\"üìÅ Using results from: {latest}\")\n",
    "    \n",
    "    # Generate plots and tables\n",
    "    print(\"\\nüìä Generating plots and LaTeX tables...\")\n",
    "    !python results_analysis.py {latest}/final_results.json\n",
    "    \n",
    "    print(f\"\\n‚úì Results saved to: {latest}/analysis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. View results summary\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "results_file = latest / \"final_results.json\"\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Create comparison table\n",
    "    if 'summary' in results and 'comparison_table' in results['summary']:\n",
    "        df = pd.DataFrame(results['summary']['comparison_table'])\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"FINAL RESULTS COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nOverall SCP Scores (higher is better):\")\n",
    "        print(df[['method', 'overall_score']].to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\\nDetailed Metrics:\")\n",
    "        print(df[['method', 'semantic_consistency', 'contradiction_rate', \n",
    "                  'forgetting', 'accuracy']].to_string(index=False))\n",
    "        \n",
    "        # Identify best method\n",
    "        best_idx = df['overall_score'].idxmax()\n",
    "        best_method = df.loc[best_idx, 'method']\n",
    "        best_score = df.loc[best_idx, 'overall_score']\n",
    "        \n",
    "        print(f\"\\nüèÜ Best method: {best_method} (SCP Score: {best_score:.4f})\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Comparison table not found in results\")\n",
    "else:\n",
    "    print(\"‚ùå Results file not found. Run cell 5 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb56857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Display plots\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "analysis_dir = latest / \"analysis\"\n",
    "\n",
    "if analysis_dir.exists():\n",
    "    plot_files = [\n",
    "        'overall_comparison.png',\n",
    "        'metrics_radar.png',\n",
    "        'per_task_performance.png',\n",
    "        'forgetting_analysis.png'\n",
    "    ]\n",
    "    \n",
    "    for plot_file in plot_files:\n",
    "        plot_path = analysis_dir / plot_file\n",
    "        if plot_path.exists():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üìä {plot_file.replace('_', ' ').title().replace('.png', '')}\")\n",
    "            print('='*60)\n",
    "            display(Image(filename=str(plot_path)))\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {plot_file} not found\")\n",
    "else:\n",
    "    print(\"‚ùå Analysis directory not found. Run cell 6 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. View LaTeX tables for paper\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "analysis_dir = latest / \"analysis\"\n",
    "\n",
    "table_files = [\n",
    "    'table_overall_results.tex',\n",
    "    'table_detailed_metrics.tex'\n",
    "]\n",
    "\n",
    "for table_file in table_files:\n",
    "    table_path = analysis_dir / table_file\n",
    "    if table_path.exists():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìÑ {table_file}\")\n",
    "        print('='*60)\n",
    "        with open(table_path) as f:\n",
    "            print(f.read())\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {table_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e085200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Download all results as ZIP\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "\n",
    "# Create ZIP archive\n",
    "zip_name = 'sgcl_results'\n",
    "print(f\"üì¶ Creating {zip_name}.zip...\")\n",
    "shutil.make_archive(zip_name, 'zip', latest)\n",
    "\n",
    "print(f\"\\n‚úÖ Results packaged successfully!\")\n",
    "print(f\"üìÅ Archive contains:\")\n",
    "print(f\"   - Trained models (SG-CL, Naive, EWC, Replay)\")\n",
    "print(f\"   - Training statistics and metrics\")\n",
    "print(f\"   - Evaluation results (SCP scores)\")\n",
    "print(f\"   - Publication plots (PNG)\")\n",
    "print(f\"   - LaTeX tables\")\n",
    "print(f\"   - Raw JSON data\")\n",
    "print(f\"\\nüíæ Download '{zip_name}.zip' from the Output panel (right sidebar) ‚Üí\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd42338",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "**ConceptNet API errors (502 Bad Gateway):**\n",
    "- Harmless! SID automatically falls back to rule-based entity extraction\n",
    "- Does not affect training or results\n",
    "\n",
    "**EWC device mismatch error:**\n",
    "- Known issue with multi-GPU setups\n",
    "- Other 3 methods still work fine\n",
    "- Results are still valid for comparison\n",
    "\n",
    "**Out of memory:**\n",
    "- Reduce batch size in config\n",
    "- Use smaller model (gpt2 instead of phi-3)\n",
    "- Or run with `--mini` flag for smaller dataset\n",
    "\n",
    "**Evaluation fails:**\n",
    "- Training results are still saved\n",
    "- Can skip evaluation and use training metrics\n",
    "- Check `final_results.json` for available data\n",
    "\n",
    "---\n",
    "## Next Steps\n",
    "\n",
    "1. **Download results** - Use cell 10 to package everything\n",
    "2. **Write paper** - Use generated plots and LaTeX tables\n",
    "3. **Document system** - See COMPLETE_SYSTEM.md in repo\n",
    "4. **Test guardrails** - Examine SG-CL conflict detection logs\n",
    "5. **Compare methods** - Analyze per-task performance differences\n",
    "\n",
    "**For full SeCA v2.0 dataset:**\n",
    "- Contact original authors or prepare your own continual learning dataset\n",
    "- Place JSON file at `sid/seca_publication_v2.json`\n",
    "- Re-run experiments for production results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
