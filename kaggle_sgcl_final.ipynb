{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f8373a",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "**For FAST testing (15-20 min)**:\n",
    "- Run Cell 4: Quick test with mini dataset\n",
    "\n",
    "**For FULL experiments (1-2 hours)**:\n",
    "- Run Cell 5: Full 10k with GPT-2 (recommended)\n",
    "\n",
    "**For PUBLICATION results (4-6 hours)**:\n",
    "- Run Cell 6: Full 10k with Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a59216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check GPU availability\n",
    "import torch\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install dependencies\n",
    "!pip install -q transformers peft accelerate sentence-transformers matplotlib seaborn pandas scipy\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd077c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Clone SGCL project (FRESH - gets latest code + 10k dataset)\n",
    "import os\n",
    "os.chdir('/kaggle/working')\n",
    "!rm -rf SGCL  # Force remove any old cached code\n",
    "!git clone https://github.com/mithun1203/SGCL.git\n",
    "%cd SGCL\n",
    "!git log -1 --oneline  # Verify latest commit\n",
    "\n",
    "print(\"\\nâœ… Fresh code loaded!\")\n",
    "print(\"   - 10k SeCA dataset (16 tasks Ã— 625 samples)\")\n",
    "print(\"   - All bug fixes applied\")\n",
    "print(\"   - Ready for experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. QUICK TEST (~15-20 minutes)\n",
    "# Tests system with mini dataset to verify everything works\n",
    "print(\"ðŸš€ Running quick verification test...\")\n",
    "print(\"â±ï¸  Estimated time: 15-20 minutes\")\n",
    "print(\"ðŸ“Š Using: GPT-2 + Mini dataset (3 tasks)\\n\")\n",
    "\n",
    "!python run_full_experiments.py --quick\n",
    "\n",
    "print(\"\\nâœ… Quick test completed! System is working.\")\n",
    "print(\"Now run Cell 5 for full experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FULL EXPERIMENTS with GPT-2 (~1-2 hours) â­ RECOMMENDED\n",
    "# Uses 10k dataset but with GPT-2 for faster training\n",
    "print(\"ðŸš€ Starting FULL experiments with GPT-2...\")\n",
    "print(\"â±ï¸  Estimated time: 1-2 hours on GPU T4\")\n",
    "print(\"ðŸ“Š Training 4 methods on 10k dataset (16 tasks)\")\n",
    "print(\"   - SG-CL (Ours) with conflict detection\")\n",
    "print(\"   - Naive Fine-tuning\")\n",
    "print(\"   - EWC (Elastic Weight Consolidation)\")\n",
    "print(\"   - Experience Replay\\n\")\n",
    "\n",
    "!python run_full_experiments.py --model gpt2\n",
    "\n",
    "print(\"\\nâœ… Full experiments completed!\")\n",
    "print(\"Run Cell 7 to generate plots and view results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. FULL EXPERIMENTS with Phi-3 (~4-6 hours) - Optional for publication\n",
    "# Uses larger Phi-3 model for best results\n",
    "print(\"ðŸš€ Starting FULL experiments with Phi-3...\")\n",
    "print(\"â±ï¸  Estimated time: 4-6 hours on GPU T4\")\n",
    "print(\"ðŸ“Š Training 4 methods on 10k dataset (16 tasks)\\n\")\n",
    "print(\"âš ï¸  WARNING: This takes 4-6 hours!\")\n",
    "print(\"    Use Cell 5 (GPT-2) for faster results.\\n\")\n",
    "\n",
    "!python run_full_experiments.py --model microsoft/phi-3-mini-4k-instruct\n",
    "\n",
    "print(\"\\nâœ… Publication-grade experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925da13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generate publication-quality plots\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Find the latest experiment directory\n",
    "exp_dirs = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))\n",
    "if not exp_dirs:\n",
    "    print(\"âŒ No experiment results found. Run Cell 5 or 6 first.\")\n",
    "else:\n",
    "    latest = exp_dirs[-1]\n",
    "    print(f\"ðŸ“ Using results from: {latest}\")\n",
    "    \n",
    "    # Generate plots and tables\n",
    "    print(\"\\nðŸ“Š Generating plots and LaTeX tables...\")\n",
    "    !python results_analysis.py {latest}/final_results.json\n",
    "    \n",
    "    print(f\"\\nâœ“ Results saved to: {latest}/analysis/\")\n",
    "    print(\"\\nRun Cell 8 to view results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb631b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. View results summary\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "results_file = latest / \"final_results.json\"\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Create comparison table\n",
    "    if 'summary' in results and 'comparison_table' in results['summary']:\n",
    "        df = pd.DataFrame(results['summary']['comparison_table'])\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"FINAL RESULTS COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nOverall SCP Scores (higher is better):\")\n",
    "        print(df[['method', 'overall_score']].to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\\nDetailed Metrics:\")\n",
    "        print(df[['method', 'semantic_consistency', 'contradiction_rate', \n",
    "                  'forgetting', 'accuracy']].to_string(index=False))\n",
    "        \n",
    "        # Identify best method\n",
    "        best_idx = df['overall_score'].idxmax()\n",
    "        best_method = df.loc[best_idx, 'method']\n",
    "        best_score = df.loc[best_idx, 'overall_score']\n",
    "        \n",
    "        print(f\"\\nðŸ† Best method: {best_method} (SCP Score: {best_score:.4f})\")\n",
    "        print(\"\\nRun Cell 9 to display plots.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Comparison table not found in results\")\n",
    "else:\n",
    "    print(\"âŒ Results file not found. Run Cell 5 or 6 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Display plots\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "analysis_dir = latest / \"analysis\"\n",
    "\n",
    "if analysis_dir.exists():\n",
    "    plot_files = [\n",
    "        'overall_comparison.png',\n",
    "        'metrics_radar.png',\n",
    "        'per_task_performance.png',\n",
    "        'forgetting_analysis.png'\n",
    "    ]\n",
    "    \n",
    "    for plot_file in plot_files:\n",
    "        plot_path = analysis_dir / plot_file\n",
    "        if plot_path.exists():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ðŸ“Š {plot_file.replace('_', ' ').title().replace('.png', '')}\")\n",
    "            print('='*60)\n",
    "            display(Image(filename=str(plot_path)))\n",
    "        else:\n",
    "            print(f\"âš ï¸ {plot_file} not found\")\n",
    "else:\n",
    "    print(\"âŒ Analysis directory not found. Run Cell 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. View LaTeX tables (for copy-paste into paper)\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "analysis_dir = latest / \"analysis\"\n",
    "\n",
    "table_files = [\n",
    "    'table_overall_results.tex',\n",
    "    'table_detailed_metrics.tex'\n",
    "]\n",
    "\n",
    "for table_file in table_files:\n",
    "    table_path = analysis_dir / table_file\n",
    "    if table_path.exists():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ“„ {table_file}\")\n",
    "        print('='*60)\n",
    "        with open(table_path) as f:\n",
    "            print(f.read())\n",
    "    else:\n",
    "        print(f\"âš ï¸ {table_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd308a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Download all results as ZIP\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "latest = sorted(Path(\"experiments\").glob(\"full_experiment_*\"))[-1]\n",
    "\n",
    "# Create ZIP archive\n",
    "zip_name = 'sgcl_results'\n",
    "print(f\"ðŸ“¦ Creating {zip_name}.zip...\")\n",
    "shutil.make_archive(zip_name, 'zip', latest)\n",
    "\n",
    "print(f\"\\nâœ… Results packaged successfully!\")\n",
    "print(f\"ðŸ“ Archive contains:\")\n",
    "print(f\"   - Trained models (SG-CL, Naive, EWC, Replay)\")\n",
    "print(f\"   - Training statistics and metrics\")\n",
    "print(f\"   - Evaluation results (SCP scores)\")\n",
    "print(f\"   - Publication plots (PNG)\")\n",
    "print(f\"   - LaTeX tables\")\n",
    "print(f\"   - Raw JSON data\")\n",
    "print(f\"\\nðŸ’¾ Download '{zip_name}.zip' from the Output panel (right sidebar) â†’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698610b",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š Dataset Information\n",
    "\n",
    "**SeCA v2.0-10k Dataset**\n",
    "- **Total**: 10,000 samples across 16 sequential tasks\n",
    "- **Per-task**: 625 samples (500 train / 125 test)\n",
    "- **Conflict rate**: 48.2% (target: 30-50%)\n",
    "- **Quality**: Hybrid (320 manual core + 9,680 systematic augmentation)\n",
    "\n",
    "**Schema**:\n",
    "```json\n",
    "{\n",
    "  \"task_id\": 7,\n",
    "  \"sentence\": \"Penguins cannot fly.\",\n",
    "  \"label\": \"conflict\",\n",
    "  \"conflict_type\": \"exception_violation\",\n",
    "  \"conflicts_with\": [\"Birds can fly.\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**Train/Test Split**: Dynamic 80/20 split per task (seed=42)\n",
    "\n",
    "---\n",
    "## ðŸŽ“ Knowledge Base\n",
    "\n",
    "**ConceptNet v5.7 Integration**:\n",
    "- **Concepts**: 142,190 high-quality\n",
    "- **Relations**: 321,089 semantic edges\n",
    "- **Operation**: Fully offline (no internet needed)\n",
    "- **Coverage**: 1000Ã— larger than baseline\n",
    "\n",
    "---\n",
    "## âš™ï¸ Runtime Estimates\n",
    "\n",
    "| Configuration | Dataset | Time |\n",
    "|---------------|---------|------|\n",
    "| Quick Test | Mini (3 tasks) | 15-20 min |\n",
    "| **GPT-2 Full** â­ | 10k (16 tasks) | **1-2 hours** |\n",
    "| Phi-3 Full | 10k (16 tasks) | 4-6 hours |\n",
    "\n",
    "**Recommendation**: Use GPT-2 for faster results. Results are comparable.\n",
    "\n",
    "---\n",
    "## ðŸŽ¯ Citation (For Report)\n",
    "\n",
    "> \"We evaluate on SeCA v2.0-10k, a semantic consistency benchmark containing 16 sequential tasks with 625 samples each (10,000 total). The dataset employs a hybrid approach: 320 manually curated core samples with complete semantic annotations, augmented with 9,680 systematically generated samples maintaining a 48% conflict rate. Each task is split 80/20 into train/test sets.\"\n",
    "\n",
    "---\n",
    "## ðŸ› Troubleshooting\n",
    "\n",
    "**Out of memory?**\n",
    "- Restart kernel and run again\n",
    "- Use GPT-2 instead of Phi-3\n",
    "\n",
    "**Taking too long?**\n",
    "- GPT-2 is 3-4Ã— faster than Phi-3\n",
    "- Quick test (Cell 4) verifies everything works\n",
    "\n",
    "**Errors during training?**\n",
    "- Check GPU is enabled (Settings â†’ Accelerator â†’ GPU)\n",
    "- Verify internet connection for model download\n",
    "\n",
    "**EWC fails?**\n",
    "- Known issue with multi-GPU setups\n",
    "- Other 3 methods still produce valid results\n",
    "\n",
    "---\n",
    "## âœ… Expected Results\n",
    "\n",
    "After running Cell 5 or 6, you should see:\n",
    "\n",
    "1. **Training logs** for 4 methods\n",
    "2. **SCP scores** showing SG-CL > Baselines\n",
    "3. **4 plots** (comparison, radar, per-task, forgetting)\n",
    "4. **2 LaTeX tables** ready for paper\n",
    "5. **ZIP file** with all results\n",
    "\n",
    "**Typical SCP Scores**:\n",
    "- SG-CL: 0.75-0.85\n",
    "- EWC: 0.65-0.75\n",
    "- Replay: 0.60-0.70\n",
    "- Naive: 0.45-0.55\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Run Cell 1 to start! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
